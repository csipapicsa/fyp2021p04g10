{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import difflib\n",
    "from nltk import agreement\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random #As of now only used for generating 100 random tweets for manual labelling\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "### The Offensive Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../data/raw/offensive/train_text.txt\", 'r', encoding = \"utf-8\")\n",
    "inputlist = [line for line in f]\n",
    "f.close()\n",
    "\n",
    "with open(\"../data/raw/offensive/train_labels.txt\", 'r', encoding = \"utf-8\") as f:\n",
    "    offensive_labels = [int(i.strip(\"\\n\")) for i in f]\n",
    "\n",
    "\n",
    "\n",
    "off_training_data, off_validation_data = inputlist[:len(inputlist)//2], inputlist[len(inputlist)//2:]\n",
    "off_training_labels, off_validation_labels = offensive_labels[:len(inputlist)//2], offensive_labels[len(inputlist)//2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer as Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## patterns\n",
    "def tokenizer(tweets):\n",
    "    \"\"\"\n",
    "    Function that takes a list of strings and returns the tokenized version of each string\n",
    "    \"\"\"\n",
    "\n",
    "    token_pat = re.compile(r'[\\w@’#]+')\n",
    "    skippable_pat = re.compile(r'\\s+')\n",
    "\n",
    "    non_white_space = re.compile(r'[^@’#\\w\\s]') #Finds characters that are not white_space nor word characters (nor @’#)\n",
    "\n",
    "\n",
    "    tokenlist = []\n",
    "    for i in tweets:\n",
    "        tokens = []\n",
    "        unmatchable = []\n",
    "        line = i\n",
    "        while line:\n",
    "            skippable_match = re.search(skippable_pat, line)\n",
    "            nws_match = re.search(non_white_space, line) #Search for non-word && non-whitespace chars (nws = non_white_space)\n",
    "            if skippable_match and skippable_match.start() == 0:\n",
    "                # If there is one at the beginning of the line, just skip it.\n",
    "                line = line[skippable_match.end():]\n",
    "\n",
    "            elif nws_match and nws_match.start() == 0: # If a character is neither non_white_space nor a word-character\n",
    "                tokens.append(line[:nws_match.end()]) #Append it to tokens\n",
    "                line = line[nws_match.end():] #Move further along in line\n",
    "            else:\n",
    "                # Else try finding a real token.\n",
    "                token_match = re.search(token_pat, line)\n",
    "                if token_match and token_match.start() == 0:\n",
    "                    # If there is one at the beginning of the line, tokenise it.\n",
    "                    tokens.append(line[:token_match.end()])\n",
    "                    line = line[token_match.end():]\n",
    "                else:\n",
    "                    # Else there is unmatchable material here.\n",
    "                    # It ends where a skippable or token match starts, or at the end of the line.\n",
    "                    unmatchable_end = len(line)\n",
    "                    if skippable_match:\n",
    "                        unmatchable_end = skippable_match.start()\n",
    "                    if token_match:\n",
    "                        unmatchable_end = min(unmatchable_end, token_match.start())\n",
    "                    # Add it to unmatchable and discard from line.\n",
    "                    unmatchable.append(line[:unmatchable_end])\n",
    "                    line = line[unmatchable_end:]\n",
    "        tokenlist.append(tokens)\n",
    "    return(tokenlist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_tweets = tokenizer(off_training_data)\n",
    "#[print(*i) for i in token_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TweetTokenizer Initialisation\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "j = 0\n",
    "for i in off_training_data:\n",
    "    temp = i\n",
    "    diff = difflib.context_diff(tknzr.tokenize(i),token_tweets[j])\n",
    "    #print(\"\".join(diff), end = \"\")\n",
    "    print(i,\"tknzr:\",tknzr.tokenize(i),\"\\ntokenlist:\",token_tweets[j],\"\\n\")\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "### Corpus size of Offensive and sentiment training sets respectively::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wc ../data/raw/offensive/train_text.txt\n",
    "wc ../data/raw/sentiment/train_text.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Offensive:</b> 11916 lines/tweets, 262370 words <br>\n",
    "<b>Sentiment:</b> 35615 lines/tweets, 877516 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running tokenizer func on offensive and sentiment training data to get token count right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/raw/offensive/train_text.txt\", \"r\",  encoding = \"utf-8\",) as f:\n",
    "    offensive_raw = [line for line in f]\n",
    "\n",
    "with open(\"../data/raw/sentiment/train_text.txt\", \"r\",  encoding = \"utf-8\",) as f:\n",
    "    sentiment_raw = [line for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offensive_tokens = tokenizer(offensive_raw)\n",
    "sentiment_tokens = tokenizer(sentiment_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://stackoverflow.com/questions/45019607/count-occurrence-of-a-list-in-a-list-of-lists\n",
    "off_uniq = pd.Series(offensive_tokens).explode().value_counts()\n",
    "sent_uniq = pd.Series(sentiment_tokens).explode().value_counts()\n",
    "\n",
    "#Turning above pd.series into dataframes, for ease of use later\n",
    "#Transformation found at:https://stackoverflow.com/questions/40224319/pandas-series-to-dataframe-using-series-indexes-as-columns\n",
    "off_uniq = off_uniq.to_frame().reset_index()\n",
    "sent_uniq = sent_uniq.to_frame().reset_index()\n",
    "\n",
    "#Renaming columns in dataframes\n",
    "off_uniq.columns = [\"token\",\"count\"]\n",
    "sent_uniq.columns = [\"token\",\"count\"]\n",
    "\n",
    "print(\"Offensive dataset, top 10 tokens:\",\"\\n\",off_uniq[:10],\"\\n\")\n",
    "print(\"Sentiment dataset, top 10 tokens:\",\"\\n\",sent_uniq[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving tokens to file, running commandline operation on these new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_token_files():\n",
    "#     \"\"\"\n",
    "#     Function that makes offensive_tokens.txt and sentiment_tokens.txt files\n",
    "#     \"\"\"\n",
    "#     with open(\"../data/interim/tokenized/offensive_tokens.txt\",\"w\", encoding = \"utf-8\") as f:\n",
    "#         for i in offensive_tokens:\n",
    "#             string = str(i)\n",
    "#             f.write(string[1:-1]+\"\\n\")\n",
    "\n",
    "#     with open(\"../data/interim/tokenized/sentiment_tokens.txt\",\"w\", encoding = \"utf-8\") as f:\n",
    "#         for i in sentiment_tokens:\n",
    "#             string = str(i)\n",
    "#             f.write(string[1:-1]+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens and their counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tr ' ' '\\n' <../data/interim/tokenized/offensive_tokens.txt | sort | uniq -c | sort -r > ../data/interim/uniq/offensive_uniq.txt\n",
    "tr \" \" \"\\n\" <../data/interim/tokenized/sentiment_tokens.txt | sort | uniq -c | sort -r > ../data/interim/uniq/sentiment_uniq.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ten most frequent tokens in \"offensive_training\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head ../data/interim/uniq/offensive_uniq.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ten most frequent tokens in \"sentiment_training\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head ../data/interim/uniq/sentiment_uniq.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniq txt files loaded in as dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offensive uniq dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# off_uniq = pd.read_csv(\"../data/interim/uniq/offensive_uniq.txt\", sep=\"\\n\", names = [\"count\",\"token\"])\n",
    "# off_uniq = off_uniq[\"count\"].str.split(expand=True) #split the values to get count and tokens in different columns\n",
    "# off_uniq[0] = off_uniq[0].astype(int) #typecast elements in column 0 to integers\n",
    "# off_uniq.replace(\"^\\'|[',]{1,2}$\", \"\", regex=True, inplace=True)  #Regex to remove the '', in the tokens they're present\n",
    "\n",
    "# off_uniq\n",
    "\n",
    "\n",
    "# #\"^\\d{1,5}\" for start 1-5 ints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Uniq Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_uniq = pd.read_csv(\"../data/interim/uniq/sentiment_uniq.txt\", sep=\"\\n\", names = [\"count\",\"token\"])\n",
    "# sent_uniq = sent_uniq[\"count\"].str.split(expand=True) #split the values to get count and tokens in different columns\n",
    "# sent_uniq[0] = sent_uniq[0].astype(int) #typecast elements in column 0 to integers\n",
    "# sent_uniq.replace(\"^\\'|[',]{1,2}$\", \"\", regex=True, inplace=True)  # \"^\\d{1,5}\" for start 1-5 ints\n",
    "\n",
    "# sent_uniq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type/token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Types == Amount of different Tokens in dataset\n",
    "off_types = len(off_uniq[\"token\"])\n",
    "sent_types = len(sent_uniq[\"token\"])\n",
    "print(\"Offensive Types: {}\\nSentiment types: {}\\n\".format(off_types,sent_types))\n",
    "\n",
    "#Tokens == Amount of all \"Words\" in dataset\n",
    "off_token_amount = off_uniq[\"count\"].sum()\n",
    "sent_token_amount = sent_uniq[\"count\"].sum()\n",
    "print(\"Offensive tokens, amount: {}\\nSentiment tokens, amount: {}\\n\".format(off_token_amount, sent_token_amount))\n",
    "\n",
    "#Type/token ratio (=ttratio)\n",
    "off_ttratio = off_types/off_token_amount\n",
    "sent_ttratio = sent_types/sent_token_amount\n",
    "print(\"Offensive type/token ratio: {:.4f}\\nSentiment type/token ratio: {:.4f}\".format(off_ttratio, sent_ttratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens that only occur 1, 2 or 3 times\n",
    "<ul>\n",
    "    <li>Things like Hashtags and emojis are prevalent, but they, more importantly, contain most of the types/vocabulary</li>\n",
    "    <li>Tokens that occur only once make up 58% of the types in both datasets!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"offensive types w. freq 1, 2, or 3 divided by total types: {:.2f}\".format(\n",
    "    len(off_uniq.loc[(off_uniq[\"count\"]==1) | (off_uniq[\"count\"]==2) | (off_uniq[\"count\"]==3)])/off_types*100))\n",
    "print(\"Sentiment types w. freq 1, 2, or 3 divided by total types: {:.2f}\".format(\n",
    "    len(sent_uniq.loc[(sent_uniq[\"count\"]==1) | (sent_uniq[\"count\"]==2) | (sent_uniq[\"count\"]==3)])/sent_types*100))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Offensive types w. freq. just 1 divided by total types: {:.2f}\".format(len(off_uniq.loc[off_uniq[\"count\"]==1])/off_types*100))\n",
    "print(\"Sentiment types w. freq. just 1 divided by total types: {:.2f}\".format(len(sent_uniq.loc[sent_uniq[\"count\"]==1])/sent_types*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(off_uniq.loc[(off_uniq[\"count\"]==1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.displot(x=\"token\", data=\"off_uniq\")\n",
    "sns.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of types showing up x times in the offensive dataset (e.g 14000 tokens only showing up once, and so on)\n",
    "# Skipping top 500 types, for visibility in plot (They're not impactful on the plot otherwise, the most frequent of \n",
    "#     these 500 entries is 13)\n",
    "#Plotting visual and double y-axes found at https://stackoverflow.com/questions/33179122/seaborn-countplot-with-frequencies\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "sns.countplot(x=\"count\", data=off_uniq[500:]) #Sns counts the type frequency of each word, and plots it\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.title(\"Frequency of 500 types showing up x times in the Offensive dataset\")\n",
    "plt.xlabel(\"Type occuring x time(s)\")\n",
    "plt.ylabel(\"sum of types occurring x time(s)\")\n",
    "ax.tick_params('x',rotation=45, labelsize = 10) #xlabels are rotated 45 degrees and made bigger\n",
    "\n",
    "# Twin axes, creating and visualising\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel(\"Frequency (percent)\")\n",
    "\n",
    "# Moving the ticks and labels of y-axes to opposite sides for more visually pleasing plot\n",
    "ax2.yaxis.tick_left()\n",
    "ax.yaxis.tick_right()\n",
    "ax.yaxis.set_label_position('right')\n",
    "ax2.yaxis.set_label_position('left')\n",
    "\n",
    "# Setting appropriate limits for the y-axes, removing duplicate grid\n",
    "ax.set_ylim(0,len(off_uniq))\n",
    "ax2.set_ylim(0,100)\n",
    "ax2.grid(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noticable difference in the two datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Big difference in size, sentiment dataset over twice the amount of tokens (=library twice the size)</li>\n",
    "    <li>otherwise quite similar, in both sets the percentage of the vocabulary made up of tokens w. frq. 1 is ~ 56-57%</li>\n",
    "    <ul><li>Both datasets also seem to follow Zipf's law (see below graphs)</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Statistics Consistent with Zipf's law?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log-log plot of the rank of token frequency against against the frequency in the offensive dataset\n",
    "off_uniq[\"log_frq\"] = np.log(off_uniq[\"count\"])\n",
    "off_uniq[\"log_rank\"] = np.log(off_uniq[\"count\"].rank(ascending=False))\n",
    "sns.relplot(x=\"log_rank\",y=\"log_frq\", data=off_uniq, color=\"red\", edgecolor=(0.2,0,0,0.01)).set(title=\n",
    "                \"log-log plot of frequency against rank of frequency in Offensive dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log-log plot of the rank of token frequency against against the frequency in the sentiment dataset\n",
    "sent_uniq[\"log_frq\"] = np.log(sent_uniq[\"count\"])\n",
    "sent_uniq[\"log_rank\"] = np.log(sent_uniq[\"count\"].rank(ascending=False))\n",
    "sns.relplot(x=\"log_rank\",y=\"log_frq\", data=sent_uniq, color=\"r\", edgecolor=(0.2,0,0,0.01)).set(title=\n",
    "                \"log-log plot of frequency against rank of frequency in Sentiment dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>As seen in the above plots, both datasets seem consistent with Zipf's law</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Manual Annotation & Inter-user Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating 100 random tweets for manual annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42) #Seeded for consistency\n",
    "random_tweets = random.sample(list(enumerate(sentiment_raw)),100)\n",
    "rtweet_index = [i[0] for i in random_tweets]\n",
    "\n",
    "# #File-generation is commented out, as the randomness is seeded, thus Making the same \"Random\" file every time\n",
    "# with open(\"../data/interim/random_tweets.txt\",\"w\", encoding=\"utf-8\") as f:\n",
    "#     [f.write(str(i[1])+\"\\n\") for i in random_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_label = pd.read_csv('../data/raw/sentiment/train_labels.txt',header=None)\n",
    "sent_raw = pd.read_csv(\"../data/raw/sentiment/train_text.txt\",header=None, sep=\"\\n\",quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(sent_label.iloc[rtweet_index])\n",
    "# display(sent_raw.iloc[rtweet_index])\n",
    "len(sent_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating 100 random ints from the interval [0-2], for later testing\n",
    "\n",
    "# test_labels = random.choices([0,1,2], k=100)\n",
    "# with open(\"../data/interim/manual_annotation/random_test.txt\",\"w\", encoding=\"utf-8\") as f:\n",
    "#     [f.write(str(i)+\"\\n\") for i in test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the manually annotated labels into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_labels = pd.read_csv(\"../data/interim/manual_annotation/all_combined.csv\", delimiter=\",\") #All manual labels\n",
    "man_labels = man_labels.iloc[:,:-1] #Not using the _A0_value column from the file\n",
    "display(man_labels) #The manually annotated labels, put into a dataframe\n",
    "\n",
    "same_label = man_labels.eq(man_labels.iloc[:,0], axis=0).all(1) #Finding where all annotators agree on a label\n",
    "display(same_label)\n",
    "print(\"# of equal labels:\",np.sum(same_label))\n",
    "obs_agreement = np.sum(same_label)/len(man_labels.iloc[:,0])\n",
    "print(\"observed agreement:\",obs_agreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Chance-corrected agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting manual label answers to calculate Scott's pi, Fleiss' kappa with nltk.agreement\n",
    "tweets_len = len(man_labels.iloc[:,0])\n",
    "formatted_answers = [] #Formatting of only the manually annotated data\n",
    "for column in range(len(man_labels.columns)):\n",
    "    for tweet_num in range(tweets_len):\n",
    "        formatted_answers.append([column+1,tweet_num,man_labels.iloc[tweet_num,column]])\n",
    "\n",
    "#adding the \"True\" labels to all_formatted:\n",
    "all_formatted = formatted_answers.copy() #Formatting of BOTH the manually annotated data AND the \"True\" Annotation of the data\n",
    "true_label_list = list(sent_label.iloc[rtweet_index][0])\n",
    "for i in range(tweets_len):\n",
    "    all_formatted.append([len(man_labels.columns)+1,i,true_label_list[i]])\n",
    "    \n",
    "print(\"lenght of formatted_answers:\",len(formatted_answers))\n",
    "print(\"length of all_formatted:\",len(all_formatted)) #should be 100 characters longer than the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chance-corrected for just the manual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note that in the nltk.agreement documentation: https://www.nltk.org/_modules/nltk/metrics/agreement.html\n",
    "the returned value is the chance-corrected agreement, not just A_e.\n",
    "\"\"\"\n",
    "ratingtask = agreement.AnnotationTask(data=formatted_answers)\n",
    "print(\"Scott's pi: {:.4f}\\nCohen's kappa: {:.4f}\\nFleiss' kappa: {:.4f}\".format(ratingtask.pi(),ratingtask.kappa(),ratingtask.multi_kappa()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chance-corrected for both the manual labels AND the \"true\" labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_label_rating = agreement.AnnotationTask(data=all_formatted)\n",
    "print(\"Scott's pi: {:.4f}\\nCohen's kappa: {:.4f}\\nFleiss' kappa: {:.4f}\".format(all_label_rating.pi(),all_label_rating.kappa(),all_label_rating.multi_kappa()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the tweets with agreeing/disagreeing manual labels for later discussion, saved to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_label[same_label==False]\n",
    "manual_tweets = sent_raw.loc[rtweet_index]\n",
    "\n",
    "#tweets labels disagree on\n",
    "annotation_disagree = manual_tweets.iloc[np.where(same_label==False)]\n",
    "display(annotation_disagree[:10])#Showing the 10 first tweets with disagreeing manual annotation\n",
    "\n",
    "#tweets labels agree on\n",
    "annotation_agree = manual_tweets.iloc[np.where(same_label==True)]\n",
    "\n",
    "# File creation commented out\n",
    "# annotation_disagree.to_csv(\"../data/interim/man_anno_disagree.txt\", header=None, index=False)\n",
    "# annotation_agree.to_csv(\"../data/interim/man_anno_agree.txt\", header=None, index=False)\n",
    "\n",
    "man_labels[same_label==True][\"anno_1\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the 10 pairs of inter-annotator agreements possible from 5 annotators (4 manual and the \"True\" labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_and_true = man_labels.copy()\n",
    "man_and_true[\"true\"] = true_label_list\n",
    "man_and_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(man_and_true.columns)):\n",
    "    for j in range(i+1, len(man_and_true.columns)):\n",
    "        l1 = list(man_and_true.iloc[:,i])\n",
    "        l2 = list(man_and_true.iloc[:,j])\n",
    "        score = cohen_kappa_score(l1,l2)\n",
    "        if j==4:\n",
    "            print(\"Anno_{}, True:\\n\".format(i+1),score,\"\\n\")\n",
    "        else:\n",
    "            print(\"Anno_{}, Anno_{}:\\n\".format(i+1,j+1),score,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(tokenizer = lambda x: x, lowercase = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model on Offensive Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT USING OWN TOKENIZER, DONT USE IN FINAL REPORT\n",
    "WRONG_OFF = count_vec.fit_transform(offensive_raw[:len(offensive_raw)//2])\n",
    "WRONG_OFF\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through tokenized data, making it lower-case\n",
    "for index, tweet in enumerate(token_tweets):\n",
    "    token_tweets[index] = str(tweet).lower()\n",
    "    \n",
    "#Running Count_vectorizor (Pipeline for the coming commands)\n",
    "off_train = count_vec.fit_transform(token_tweets)\n",
    "off_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running tf_idf on off_train to \"balance\" tweets\n",
    "tf_idf_transformer = TfidfTransformer(use_idf = False)\n",
    "tf_off_train = tf_idf_transformer.fit_transform(off_train)\n",
    "tf_off_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SGDClassifier(loss=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_validation_tokens = tokenizer(off_validation_data)\n",
    "for index, tweet in enumerate(off_validation_tokens):\n",
    "    off_validation_tokens[index] = str(tweet).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training model\n",
    "off_clf = classifier.fit(tf_off_train, off_training_labels)\n",
    "\n",
    "#Preparing validation data\n",
    "off_pred = count_vec.transform(off_validation_tokens)\n",
    "tf_off_pred = tf_idf_transformer.transform(off_pred)\n",
    "\n",
    "#Fitting validation data over model\n",
    "off_predicted = off_clf.predict(tf_off_pred)\n",
    "\n",
    "# % of answers gotten right\n",
    "sum(off_predicted == off_validation_labels) / len(off_validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(off_validation_labels, off_predicted, target_names=[\"Not offensive\",\"Offensive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(off_validation_labels, off_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, The recall is very close to 1 for non-offensive tweets and very close to 0 for offensive tweets.<br>\n",
    "This means that the model predicts that most of the tweets are not offensive, and the only reason for our relatively high accuracy is that the training data is unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking offensive validation data compared to offensive predicted data\n",
    "np_list_off = np.array(off_predicted)\n",
    "np_off_validation = np.array(off_validation_labels)\n",
    "print(\"Number of predicted non-offensive tweets: {}\\nNumber of predicted offensive tweets: {}\\n\".format(\n",
    "len(np_list_off[np.where(np_list_off == 0)]),\n",
    "len(np_list_off[np.where(np_list_off == 1)])))\n",
    "\n",
    "print(\"Number of actual non-offensive tweets: {}\\nNumber of actual offensive tweets: {}\".format(\n",
    "len(np_off_validation[np.where(np_off_validation == 0)]),\n",
    "len(np_off_validation[np.where(np_off_validation == 1)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>\"For your final systems, I suggest you report overall Accuracy as well as Precision, Recall and F-score for all classes.\n",
    "\"</i> - Lecturer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
