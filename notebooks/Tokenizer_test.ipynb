{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import difflib\n",
    "from nltk import agreement\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random #As of now only used for generating 100 random tweets for manual labelling\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "### The Offensive Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../data/raw/offensive/train_text.txt\", 'r', encoding = \"utf-8\")\n",
    "inputlist = [line for line in f]\n",
    "f.close()\n",
    "\n",
    "with open(\"../data/raw/offensive/train_labels.txt\", 'r', encoding = \"utf-8\") as f:\n",
    "    offensive_labels = [int(i.strip(\"\\n\")) for i in f]\n",
    "\n",
    "\n",
    "\n",
    "off_training_data, off_validation_data = inputlist[:len(inputlist)//2], inputlist[len(inputlist)//2:]\n",
    "off_training_labels, off_validation_labels = offensive_labels[:len(inputlist)//2], offensive_labels[len(inputlist)//2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer as Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## patterns\n",
    "def tokenizer(tweets):\n",
    "    \"\"\"\n",
    "    Function that takes a list of strings and returns the tokenized version of each string\n",
    "    \"\"\"\n",
    "\n",
    "    token_pat = re.compile(r'[\\w@’#]+')\n",
    "    skippable_pat = re.compile(r'[\\s\\d]+|@user')\n",
    "\n",
    "    non_white_space = re.compile(r'[^@’#\\w\\s]') #Finds characters that are not white_space nor word characters (nor @’#)\n",
    "\n",
    "\n",
    "    tokenlist = []\n",
    "    for i in tweets:\n",
    "        tokens = []\n",
    "        unmatchable = []\n",
    "        line = i.lower() #Turning everything into lowercase\n",
    "        while line:\n",
    "            skippable_match = re.search(skippable_pat, line)\n",
    "            nws_match = re.search(non_white_space, line) #Search for non-word && non-whitespace chars (nws = non_white_space)\n",
    "            if skippable_match and skippable_match.start() == 0:\n",
    "                # If there is one at the beginning of the line, just skip it.\n",
    "                line = line[skippable_match.end():]\n",
    "\n",
    "            elif nws_match and nws_match.start() == 0: # If a character is neither non_white_space nor a word-character\n",
    "                tokens.append(line[:nws_match.end()]) #Append it to tokens\n",
    "                line = line[nws_match.end():] #Move further along in line\n",
    "            else:\n",
    "                # Else try finding a real token.\n",
    "                token_match = re.search(token_pat, line)\n",
    "                if token_match and token_match.start() == 0:\n",
    "                    # If there is one at the beginning of the line, tokenise it.\n",
    "                    tokens.append(line[:token_match.end()])\n",
    "                    line = line[token_match.end():]\n",
    "                else:\n",
    "                    # Else there is unmatchable material here.\n",
    "                    # It ends where a skippable or token match starts, or at the end of the line.\n",
    "                    unmatchable_end = len(line)\n",
    "                    if skippable_match:\n",
    "                        unmatchable_end = skippable_match.start()\n",
    "                    if token_match:\n",
    "                        unmatchable_end = min(unmatchable_end, token_match.start())\n",
    "                    # Add it to unmatchable and discard from line.\n",
    "                    unmatchable.append(line[:unmatchable_end])\n",
    "                    line = line[unmatchable_end:]\n",
    "        tokenlist.append(tokens)\n",
    "    return(tokenlist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_tweets = tokenizer(off_training_data)\n",
    "[print(*i) for i in token_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TweetTokenizer Initialisation\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "j = 0\n",
    "for i in off_training_data:\n",
    "    temp = i\n",
    "    diff = difflib.context_diff(tknzr.tokenize(i),token_tweets[j])\n",
    "    #print(\"\".join(diff), end = \"\")\n",
    "    print(i,\"tknzr:\",tknzr.tokenize(i),\"\\ntokenlist:\",token_tweets[j],\"\\n\")\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "### Corpus size of Offensive and sentiment training sets respectively::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wc ../data/raw/offensive/train_text.txt\n",
    "wc ../data/raw/sentiment/train_text.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Offensive:</b> 11916 lines/tweets, 262370 words <br>\n",
    "<b>Sentiment:</b> 35615 lines/tweets, 877516 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running tokenizer func on offensive and sentiment training data to get token count right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/raw/offensive/train_text.txt\", \"r\",  encoding = \"utf-8\",) as f:\n",
    "    offensive_raw = [line for line in f]\n",
    "\n",
    "with open(\"../data/raw/sentiment/train_text.txt\", \"r\",  encoding = \"utf-8\",) as f:\n",
    "    sentiment_raw = [line for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offensive_tokens = tokenizer(offensive_raw)\n",
    "sentiment_tokens = tokenizer(sentiment_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://stackoverflow.com/questions/45019607/count-occurrence-of-a-list-in-a-list-of-lists\n",
    "off_uniq = pd.Series(offensive_tokens).explode().value_counts()\n",
    "sent_uniq = pd.Series(sentiment_tokens).explode().value_counts()\n",
    "\n",
    "#Turning above pd.series into dataframes, for ease of use later\n",
    "#Transformation found at:https://stackoverflow.com/questions/40224319/pandas-series-to-dataframe-using-series-indexes-as-columns\n",
    "off_uniq = off_uniq.to_frame().reset_index()\n",
    "sent_uniq = sent_uniq.to_frame().reset_index()\n",
    "\n",
    "#Renaming columns in dataframes\n",
    "off_uniq.columns = [\"token\",\"count\"]\n",
    "sent_uniq.columns = [\"token\",\"count\"]\n",
    "\n",
    "print(\"Offensive dataset, top 10 tokens:\",\"\\n\",off_uniq[:10],\"\\n\")\n",
    "print(\"Sentiment dataset, top 10 tokens:\",\"\\n\",sent_uniq[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving tokens to file, running commandline operation on these new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_token_files():\n",
    "#     \"\"\"\n",
    "#     Function that makes offensive_tokens.txt and sentiment_tokens.txt files\n",
    "#     \"\"\"\n",
    "#     with open(\"../data/interim/tokenized/offensive_tokens.txt\",\"w\", encoding = \"utf-8\") as f:\n",
    "#         for i in offensive_tokens:\n",
    "#             string = str(i)\n",
    "#             f.write(string[1:-1]+\"\\n\")\n",
    "\n",
    "#     with open(\"../data/interim/tokenized/sentiment_tokens.txt\",\"w\", encoding = \"utf-8\") as f:\n",
    "#         for i in sentiment_tokens:\n",
    "#             string = str(i)\n",
    "#             f.write(string[1:-1]+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens and their counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tr ' ' '\\n' <../data/interim/tokenized/offensive_tokens.txt | sort | uniq -c | sort -r > ../data/interim/uniq/offensive_uniq.txt\n",
    "tr \" \" \"\\n\" <../data/interim/tokenized/sentiment_tokens.txt | sort | uniq -c | sort -r > ../data/interim/uniq/sentiment_uniq.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ten most frequent tokens in \"offensive_training\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head ../data/interim/uniq/offensive_uniq.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ten most frequent tokens in \"sentiment_training\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head ../data/interim/uniq/sentiment_uniq.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniq txt files loaded in as dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offensive uniq dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# off_uniq = pd.read_csv(\"../data/interim/uniq/offensive_uniq.txt\", sep=\"\\n\", names = [\"count\",\"token\"])\n",
    "# off_uniq = off_uniq[\"count\"].str.split(expand=True) #split the values to get count and tokens in different columns\n",
    "# off_uniq[0] = off_uniq[0].astype(int) #typecast elements in column 0 to integers\n",
    "# off_uniq.replace(\"^\\'|[',]{1,2}$\", \"\", regex=True, inplace=True)  #Regex to remove the '', in the tokens they're present\n",
    "\n",
    "# off_uniq\n",
    "\n",
    "\n",
    "# #\"^\\d{1,5}\" for start 1-5 ints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Uniq Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_uniq = pd.read_csv(\"../data/interim/uniq/sentiment_uniq.txt\", sep=\"\\n\", names = [\"count\",\"token\"])\n",
    "# sent_uniq = sent_uniq[\"count\"].str.split(expand=True) #split the values to get count and tokens in different columns\n",
    "# sent_uniq[0] = sent_uniq[0].astype(int) #typecast elements in column 0 to integers\n",
    "# sent_uniq.replace(\"^\\'|[',]{1,2}$\", \"\", regex=True, inplace=True)  # \"^\\d{1,5}\" for start 1-5 ints\n",
    "\n",
    "# sent_uniq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type/token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Types == Amount of different Tokens in dataset\n",
    "off_types = len(off_uniq[\"token\"])\n",
    "sent_types = len(sent_uniq[\"token\"])\n",
    "print(\"Offensive Types: {}\\nSentiment types: {}\\n\".format(off_types,sent_types))\n",
    "\n",
    "#Tokens == Amount of all \"Words\" in dataset\n",
    "off_token_amount = off_uniq[\"count\"].sum()\n",
    "sent_token_amount = sent_uniq[\"count\"].sum()\n",
    "print(\"Offensive tokens, amount: {}\\nSentiment tokens, amount: {}\\n\".format(off_token_amount, sent_token_amount))\n",
    "\n",
    "#Type/token ratio (=ttratio)\n",
    "off_ttratio = off_types/off_token_amount\n",
    "sent_ttratio = sent_types/sent_token_amount\n",
    "print(\"Offensive type/token ratio: {:.4f}\\nSentiment type/token ratio: {:.4f}\".format(off_ttratio, sent_ttratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens that only occur 1, 2 or 3 times\n",
    "<ul>\n",
    "    <li>Things like Hashtags and emojis are prevalent, but they, more importantly, contain most of the types/vocabulary</li>\n",
    "    <li>Tokens that occur only once make up 58% of the types in both datasets!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"offensive types w. freq 1, 2, or 3 divided by total types: {:.2f}\".format(\n",
    "    len(off_uniq.loc[(off_uniq[\"count\"]==1) | (off_uniq[\"count\"]==2) | (off_uniq[\"count\"]==3)])/off_types*100))\n",
    "print(\"Sentiment types w. freq 1, 2, or 3 divided by total types: {:.2f}\".format(\n",
    "    len(sent_uniq.loc[(sent_uniq[\"count\"]==1) | (sent_uniq[\"count\"]==2) | (sent_uniq[\"count\"]==3)])/sent_types*100))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Offensive types w. freq. just 1 divided by total types: {:.2f}\".format(len(off_uniq.loc[off_uniq[\"count\"]==1])/off_types*100))\n",
    "print(\"Sentiment types w. freq. just 1 divided by total types: {:.2f}\".format(len(sent_uniq.loc[sent_uniq[\"count\"]==1])/sent_types*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(off_uniq.loc[(off_uniq[\"count\"]==1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.displot(x=\"token\", data=\"off_uniq\")\n",
    "sns.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of types showing up x times in the offensive dataset (e.g 14000 tokens only showing up once, and so on)\n",
    "# Skipping top 500 types, for visibility in plot (They're not impactful on the plot otherwise, the most frequent of \n",
    "#     these 500 entries is 13)\n",
    "#Plotting visual and double y-axes found at https://stackoverflow.com/questions/33179122/seaborn-countplot-with-frequencies\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "sns.countplot(x=\"count\", data=off_uniq[500:]) #Sns counts the type frequency of each word, and plots it\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.title(\"Frequency of 500 types showing up x times in the Offensive dataset\")\n",
    "plt.xlabel(\"Type occuring x time(s)\")\n",
    "plt.ylabel(\"sum of types occurring x time(s)\")\n",
    "ax.tick_params('x',rotation=45, labelsize = 10) #xlabels are rotated 45 degrees and made bigger\n",
    "\n",
    "# Twin axes, creating and visualising\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel(\"Frequency (percent)\")\n",
    "\n",
    "# Moving the ticks and labels of y-axes to opposite sides for more visually pleasing plot\n",
    "ax2.yaxis.tick_left()\n",
    "ax.yaxis.tick_right()\n",
    "ax.yaxis.set_label_position('right')\n",
    "ax2.yaxis.set_label_position('left')\n",
    "\n",
    "# Setting appropriate limits for the y-axes, removing duplicate grid\n",
    "ax.set_ylim(0,len(off_uniq))\n",
    "ax2.set_ylim(0,100)\n",
    "ax2.grid(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noticable difference in the two datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Big difference in size, sentiment dataset over twice the amount of tokens (=library twice the size)</li>\n",
    "    <li>otherwise quite similar, in both sets the percentage of the vocabulary made up of tokens w. frq. 1 is ~ 56-57%</li>\n",
    "    <ul><li>Both datasets also seem to follow Zipf's law (see below graphs)</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Statistics Consistent with Zipf's law?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log-log plot of the rank of token frequency against against the frequency in the offensive dataset\n",
    "off_uniq[\"log_frq\"] = np.log(off_uniq[\"count\"])\n",
    "off_uniq[\"log_rank\"] = np.log(off_uniq[\"count\"].rank(ascending=False))\n",
    "sns.relplot(x=\"log_rank\",y=\"log_frq\", data=off_uniq, color=\"red\", edgecolor=(0.2,0,0,0.01)).set(title=\n",
    "                \"log-log plot of frequency against rank of frequency in Offensive dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log-log plot of the rank of token frequency against against the frequency in the sentiment dataset\n",
    "sent_uniq[\"log_frq\"] = np.log(sent_uniq[\"count\"])\n",
    "sent_uniq[\"log_rank\"] = np.log(sent_uniq[\"count\"].rank(ascending=False))\n",
    "sns.relplot(x=\"log_rank\",y=\"log_frq\", data=sent_uniq, color=\"r\", edgecolor=(0.2,0,0,0.01)).set(title=\n",
    "                \"log-log plot of frequency against rank of frequency in Sentiment dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>As seen in the above plots, both datasets seem consistent with Zipf's law</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Manual Annotation & Inter-user Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating 100 random tweets for manual annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42) #Seeded for consistency\n",
    "random_tweets = random.sample(list(enumerate(sentiment_raw)),100)\n",
    "rtweet_index = [i[0] for i in random_tweets]\n",
    "\n",
    "# #File-generation is commented out, as the randomness is seeded, thus Making the same \"Random\" file every time\n",
    "# with open(\"../data/interim/random_tweets.txt\",\"w\", encoding=\"utf-8\") as f:\n",
    "#     [f.write(str(i[1])+\"\\n\") for i in random_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_label = pd.read_csv('../data/raw/sentiment/train_labels.txt',header=None)\n",
    "sent_raw = pd.read_csv(\"../data/raw/sentiment/train_text.txt\",header=None, sep=\"\\n\",quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(sent_label.iloc[rtweet_index])\n",
    "# display(sent_raw.iloc[rtweet_index])\n",
    "len(sent_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating 100 random ints from the interval [0-2], for later testing\n",
    "\n",
    "# test_labels = random.choices([0,1,2], k=100)\n",
    "# with open(\"../data/interim/manual_annotation/random_test.txt\",\"w\", encoding=\"utf-8\") as f:\n",
    "#     [f.write(str(i)+\"\\n\") for i in test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the manually annotated labels into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_labels = pd.read_csv(\"../data/interim/manual_annotation/all_combined.csv\", delimiter=\",\") #All manual labels\n",
    "man_labels = man_labels.iloc[:,:-1] #Not using the _A0_value column from the file\n",
    "display(man_labels) #The manually annotated labels, put into a dataframe\n",
    "\n",
    "same_label = man_labels.eq(man_labels.iloc[:,0], axis=0).all(1) #Finding where all annotators agree on a label\n",
    "display(same_label)\n",
    "print(\"# of equal labels:\",np.sum(same_label))\n",
    "obs_agreement = np.sum(same_label)/len(man_labels.iloc[:,0])\n",
    "print(\"observed agreement:\",obs_agreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Chance-corrected agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting manual label answers to calculate Scott's pi, Fleiss' kappa with nltk.agreement\n",
    "tweets_len = len(man_labels.iloc[:,0])\n",
    "formatted_answers = [] #Formatting of only the manually annotated data\n",
    "for column in range(len(man_labels.columns)):\n",
    "    for tweet_num in range(tweets_len):\n",
    "        formatted_answers.append([column+1,tweet_num,man_labels.iloc[tweet_num,column]])\n",
    "\n",
    "#adding the \"True\" labels to all_formatted:\n",
    "all_formatted = formatted_answers.copy() #Formatting of BOTH the manually annotated data AND the \"True\" Annotation of the data\n",
    "true_label_list = list(sent_label.iloc[rtweet_index][0])\n",
    "for i in range(tweets_len):\n",
    "    all_formatted.append([len(man_labels.columns)+1,i,true_label_list[i]])\n",
    "    \n",
    "print(\"lenght of formatted_answers:\",len(formatted_answers))\n",
    "print(\"length of all_formatted:\",len(all_formatted)) #should be 100 characters longer than the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chance-corrected for just the manual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note that in the nltk.agreement documentation: https://www.nltk.org/_modules/nltk/metrics/agreement.html\n",
    "the returned value is the chance-corrected agreement, not just A_e.\n",
    "\"\"\"\n",
    "ratingtask = agreement.AnnotationTask(data=formatted_answers)\n",
    "print(\"Scott's pi: {:.4f}\\nCohen's kappa: {:.4f}\\nFleiss' kappa: {:.4f}\".format(ratingtask.pi(),ratingtask.kappa(),ratingtask.multi_kappa()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chance-corrected for both the manual labels AND the \"true\" labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_label_rating = agreement.AnnotationTask(data=all_formatted)\n",
    "print(\"Scott's pi: {:.4f}\\nCohen's kappa: {:.4f}\\nFleiss' kappa: {:.4f}\".format(all_label_rating.pi(),all_label_rating.kappa(),all_label_rating.multi_kappa()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the tweets with agreeing/disagreeing manual labels for later discussion, saved to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_label[same_label==False]\n",
    "manual_tweets = sent_raw.loc[rtweet_index]\n",
    "\n",
    "#tweets labels disagree on\n",
    "annotation_disagree = manual_tweets.iloc[np.where(same_label==False)]\n",
    "display(annotation_disagree[:10])#Showing the 10 first tweets with disagreeing manual annotation\n",
    "\n",
    "#tweets labels agree on\n",
    "annotation_agree = manual_tweets.iloc[np.where(same_label==True)]\n",
    "\n",
    "# File creation commented out\n",
    "# annotation_disagree.to_csv(\"../data/interim/man_anno_disagree.txt\", header=None, index=False)\n",
    "# annotation_agree.to_csv(\"../data/interim/man_anno_agree.txt\", header=None, index=False)\n",
    "\n",
    "man_labels[same_label==True][\"anno_1\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the 10 pairs of inter-annotator agreements possible from 5 annotators (4 manual and the \"True\" labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_and_true = man_labels.copy()\n",
    "man_and_true[\"true\"] = true_label_list\n",
    "man_and_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_np = np.empty((5,5))\n",
    "\n",
    "for i in range(len(man_and_true.columns)):\n",
    "    for j in range(i, len(man_and_true.columns)):\n",
    "        l1 = list(man_and_true.iloc[:,i])\n",
    "        l2 = list(man_and_true.iloc[:,j])\n",
    "        score = cohen_kappa_score(l1,l2)\n",
    "        annotator_np[i,j] = score\n",
    "        annotator_np[j,i] = score\n",
    "#         if j==4:\n",
    "#             print(\"Anno_{}, True:\\n\".format(i+1),score,\"\\n\")\n",
    "#         else:\n",
    "#             print(\"Anno_{}, Anno_{}:\\n\".format(i+1,j+1),score,\"\\n\")\n",
    "\n",
    "print(annotator_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# im = ax.imshow(annotator_np) #Creates heatmap of above matrix\n",
    "\n",
    "# # Shows all ticks\n",
    "# ax.set_xticks(range(5))\n",
    "# ax.set_yticks(range(5))\n",
    "# # Labels ticks\n",
    "# ax.set_xticklabels([\"Annotator 1\",\"Annotator 2\",'Annotator 3',\"Annotator 4\",\"'True' labels\"])\n",
    "# ax.set_yticklabels([\"Annotator 1\",\"Annotator 2\",'Annotator 3',\"Annotator 4\",\"'True' labels\"])\n",
    "\n",
    "# # Rotate the tick labels and set their alignment.\n",
    "# plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "#          rotation_mode=\"anchor\")\n",
    "\n",
    "# # Loop over data dimensions and create text annotations.\n",
    "# for i in range(5):\n",
    "#     for j in range(5):\n",
    "#         text = ax.text(j, i, annotator_np[i, j],\n",
    "#                        ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "# plt.grid(None)\n",
    "\n",
    "sns.heatmap(data=annotator_np, annot=True,\n",
    "            xticklabels= [\"Annotator 1\",\"Annotator 2\",'Annotator 3',\"Annotator 4\",\"'True' labels\"],\n",
    "           yticklabels = [\"Annotator 1\",\"Annotator 2\",'Annotator 3',\"Annotator 4\",\"'True' labels\"])\n",
    "ax.tick_params('x',rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(tokenizer = lambda x: x, lowercase = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model on Offensive Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT USING OWN TOKENIZER, DONT USE IN FINAL REPORT\n",
    "WRONG_OFF = count_vec.fit_transform(offensive_raw)\n",
    "WRONG_OFF\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_(classification_task, file_name):\n",
    "    with open(\"../data/raw/\"+classification_task+\"/\"+file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        temp = [int(line.strip(\"\\n\")) for line in f]\n",
    "    return(temp)\n",
    "\n",
    "def import_and_tokenize(classification_task, file_name):\n",
    "    with open(\"../data/raw/\"+classification_task+\"/\"+file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        temp = [line for line in f]\n",
    "    return(tokenizer(temp))\n",
    "\n",
    "#Loading in offensive x-train, x-test, y-train, y-test\n",
    "\n",
    "# x-train\n",
    "ox_train = import_and_tokenize(\"offensive\", \"train_text.txt\")\n",
    "\n",
    "# x-test\n",
    "ox_test = import_(\"offensive\", \"train_labels.txt\")\n",
    "\n",
    "# y-train\n",
    "oy_train = import_and_tokenize(\"offensive\", \"test_text.txt\")\n",
    "\n",
    "# y-test\n",
    "oy_test = import_(\"offensive\", \"test_labels.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ox_train))\n",
    "print(len(ox_test))\n",
    "print(len(oy_train))\n",
    "print(len(oy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running Count_vectorizor (Pipeline for the coming commands)\n",
    "count_ox_train = count_vec.fit_transform(ox_train)\n",
    "count_ox_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running tf_idf on off_train to \"balance\" tweets\n",
    "tf_idf_transformer = TfidfTransformer(use_idf = False)\n",
    "tf_off_train = tf_idf_transformer.fit_transform(count_ox_train)\n",
    "tf_off_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SGDClassifier(loss=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training model\n",
    "off_clf = classifier.fit(tf_off_train, ox_test)\n",
    "\n",
    "#Preparing validation data\n",
    "off_pred = count_vec.transform(oy_train)\n",
    "tf_off_pred = tf_idf_transformer.transform(off_pred)\n",
    "\n",
    "#Fitting validation data over model\n",
    "off_predicted = off_clf.predict(tf_off_pred)\n",
    "\n",
    "# % of answers gotten right\n",
    "sum((off_predicted == oy_test)) / len(oy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(off_predicted, oy_test, target_names=[\"Not offensive\",\"Offensive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(oy_test, off_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, The recall is very close to 1 for non-offensive tweets and very close to 0 for offensive tweets.<br>\n",
    "This means that the model predicts that most of the tweets are not offensive, and the only reason for our relatively high accuracy is that the training data is unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking offensive validation data compared to offensive predicted data\n",
    "np_list_off = np.array(off_predicted)\n",
    "np_off_validation = np.array(oy_test)\n",
    "np_off_train = np.array(ox_test)\n",
    "\n",
    "print(\"Number of predicted non-offensive tweets: {}\\nNumber of predicted offensive tweets: {}\\n\".format(\n",
    "len(np_list_off[np.where(np_list_off == 0)]),\n",
    "len(np_list_off[np.where(np_list_off == 1)])))\n",
    "\n",
    "print(\"Number of actual non-offensive tweets: {}\\nNumber of actual offensive tweets: {}\\n\".format(\n",
    "len(np_off_validation[np.where(np_off_validation == 0)]),\n",
    "len(np_off_validation[np.where(np_off_validation == 1)])))\n",
    "\n",
    "print(\"Number of training non-offensive tweets: {}\\nNumber of training offensive tweets: {}\".format(\n",
    "len(np_off_train[np.where(np_off_train == 0)]),\n",
    "len(np_off_train[np.where(np_off_train == 1)])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models for the offensive dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_clf_stats(predicted, test, classification_task):\n",
    "    name_dict = {\"offensive\": [\"Not offensive\",\"Offensive\"], \"sentiment\": [\"Negative\", \"Neutral\", \"Positive\"]}\n",
    "    #print(\"Accuracy:\",np.mean(predicted==test),\"\\n\")\n",
    "    print(metrics.accuracy_score(test, predicted))\n",
    "#     print(\"Negative Accuracy: {}      Neutral Accuracy: {}      Positive Accuracy: {}\\n\n",
    "#         Average Accuracy: {}\").format()\n",
    "    print(metrics.classification_report(predicted, test, target_names=name_dict[classification_task]),\"\\n\")\n",
    "    print(metrics.confusion_matrix(test, predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline for sgdclassifier\n",
    "sgd_clf = Pipeline([\n",
    "     ('vec', CountVectorizer(tokenizer = lambda x: x, lowercase = False, ngram_range=(1,3),\n",
    "                            max_df = 0.7, min_df = 4, max_features = 1000)),\n",
    "     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "     ('clf', SGDClassifier(loss=\"log\")),\n",
    "])\n",
    "\n",
    "sgd_clf.fit(ox_train, ox_test)\n",
    "sgd_predicted2 = text_clf.predict(oy_train)\n",
    "sgd_predicted2\n",
    "\n",
    "report_clf_stats(sgd_predicted2, oy_test, \"offensive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Highest Achieved accuracy score for SGDClassifier: 78.6% </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB\n",
    "multinb_clf = Pipeline([\n",
    "     ('vec', CountVectorizer(tokenizer = lambda x: x, lowercase = False)),\n",
    "     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "     ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "multinb_clf.fit(ox_train, ox_test)\n",
    "multinb_predict = multinb_clf.predict(oy_train)\n",
    "\n",
    "report_clf_stats(multinb_predict, oy_test, \"offensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ComplementNB\n",
    "complement_clf = Pipeline([\n",
    "     ('vec', CountVectorizer(tokenizer = lambda x: x, lowercase = False)),\n",
    "     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "     ('clf', ComplementNB()),\n",
    "])\n",
    "\n",
    "complement_clf.fit(ox_train, ox_test)\n",
    "complement_predict = complement_clf.predict(oy_train)\n",
    "\n",
    "report_clf_stats(complement_predict, oy_test, \"offensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "SVC_clf = Pipeline([\n",
    "     ('vec', CountVectorizer(tokenizer = lambda x: x, lowercase = False)),\n",
    "     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "     ('clf', SVC(kernel='poly', degree = 3)),\n",
    "])\n",
    "\n",
    "SVC_clf.fit(ox_train, ox_test)\n",
    "SVC_predict = SVC_clf.predict(oy_train)\n",
    "\n",
    "report_clf_stats(SVC_predict, oy_test, \"offensive\")\n",
    "\n",
    "#For SVC choose kernel = 'poly', degree=2 (or 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>\"For your final systems, I suggest you report overall Accuracy as well as Precision, Recall and F-score for all classes.\n",
    "\"</i> - Lecturer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers for the sentiment (multiclass) task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the following: Bayes, complement, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Sentiment x-train, x-test, y-train, y-test\n",
    "\n",
    "# x-train\n",
    "sx_train = import_and_tokenize(\"sentiment\", \"train_text.txt\")\n",
    "\n",
    "# x-test\n",
    "sx_test = import_(\"sentiment\", \"train_labels.txt\")\n",
    "\n",
    "# y-train\n",
    "sy_train = import_and_tokenize(\"sentiment\", \"test_text.txt\")\n",
    "\n",
    "# y-test\n",
    "sy_test = import_(\"sentiment\", \"test_labels.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes on Sentiment analysis\n",
    "\n",
    "multinb_clf.fit(sx_train, sx_test)\n",
    "sent_multinb = multinb_clf.predict(sy_train)\n",
    "\n",
    "report_clf_stats(sent_multinb, sy_test, \"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complement on Sentiment\n",
    "\n",
    "complement_clf.fit(sx_train, sx_test)\n",
    "sent_complement = complement_clf.predict(sy_train)\n",
    "\n",
    "report_clf_stats(sent_complement, sy_test, \"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD on Sentiment\n",
    "\n",
    "sgd_clf.fit(sx_train, sx_test)\n",
    "sent_sgd = sgd_clf.predict(sy_train)\n",
    "\n",
    "report_clf_stats(sent_sgd, sy_test, \"sentiment\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
