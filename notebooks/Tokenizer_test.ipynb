{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import difflib\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random #As of now only used for generating 100 random tweets for manual labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "### The Offensive Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../data/raw/offensive/train_text.txt\", 'r', encoding = \"utf-8\")\n",
    "inputlist = [line for line in f]\n",
    "f.close()\n",
    "\n",
    "training_data, validation_data = inputlist[:len(inputlist)//2], inputlist[len(inputlist)//2:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer as Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## patterns\n",
    "def tokenizer(tweets):\n",
    "    \"\"\"\n",
    "    Function that takes a list of strings and returns the tokenized version of each string\n",
    "    \"\"\"\n",
    "\n",
    "    token_pat = re.compile(r'[\\w@’#]+')\n",
    "    skippable_pat = re.compile(r'\\s+')\n",
    "\n",
    "    non_white_space = re.compile(r'[^@’#\\w\\s]') #Finds characters that are not white_space nor word characters (nor @’#)\n",
    "\n",
    "\n",
    "    tokenlist = []\n",
    "    for i in tweets:\n",
    "        tokens = []\n",
    "        unmatchable = []\n",
    "        line = i\n",
    "        while line:\n",
    "            skippable_match = re.search(skippable_pat, line)\n",
    "            nws_match = re.search(non_white_space, line) #Search for non-word && non-whitespace chars (nws = non_white_space)\n",
    "            if skippable_match and skippable_match.start() == 0:\n",
    "                # If there is one at the beginning of the line, just skip it.\n",
    "                line = line[skippable_match.end():]\n",
    "\n",
    "            elif nws_match and nws_match.start() == 0: # If a character is neither non_white_space nor a word-character\n",
    "                tokens.append(line[:nws_match.end()]) #Append it to tokens\n",
    "                line = line[nws_match.end():] #Move further along in line\n",
    "            else:\n",
    "                # Else try finding a real token.\n",
    "                token_match = re.search(token_pat, line)\n",
    "                if token_match and token_match.start() == 0:\n",
    "                    # If there is one at the beginning of the line, tokenise it.\n",
    "                    tokens.append(line[:token_match.end()])\n",
    "                    line = line[token_match.end():]\n",
    "                else:\n",
    "                    # Else there is unmatchable material here.\n",
    "                    # It ends where a skippable or token match starts, or at the end of the line.\n",
    "                    unmatchable_end = len(line)\n",
    "                    if skippable_match:\n",
    "                        unmatchable_end = skippable_match.start()\n",
    "                    if token_match:\n",
    "                        unmatchable_end = min(unmatchable_end, token_match.start())\n",
    "                    # Add it to unmatchable and discard from line.\n",
    "                    unmatchable.append(line[:unmatchable_end])\n",
    "                    line = line[unmatchable_end:]\n",
    "        tokenlist.append(tokens)\n",
    "    return(tokenlist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_tweets = tokenizer(training_data)\n",
    "#[print(*i) for i in token_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TweetTokenizer Initialisation\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "j = 0\n",
    "for i in training_data: \n",
    "    temp = i\n",
    "    diff = difflib.context_diff(tknzr.tokenize(i),token_tweets[j])\n",
    "    #print(\"\".join(diff), end = \"\")\n",
    "    print(i,\"tknzr:\",tknzr.tokenize(i),\"\\ntokenlist:\",token_tweets[j],\"\\n\")\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "### Corpus size of Offensive and sentiment training sets respectively::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wc ../data/raw/offensive/train_text.txt\n",
    "wc ../data/raw/sentiment/train_text.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Offensive:</b> 11916 lines/tweets, 262370 words <br>\n",
    "<b>Sentiment:</b> 35615 lines/tweets, 877516 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running tokenizer func on offensive and sentiment training data to get token count right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/raw/offensive/train_text.txt\", \"r\",  encoding = \"utf-8\",) as f:\n",
    "#     offensive_raw = [line for line in f]\n",
    "\n",
    "# with open(\"../data/raw/sentiment/train_text.txt\", \"r\",  encoding = \"utf-8\",) as f:\n",
    "#     sentiment_raw = [line for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offensive_tokens = tokenizer(offensive_raw)\n",
    "sentiment_tokens = tokenizer(sentiment_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving tokens to file, running commandline operation on these new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_token_files():\n",
    "    with open(\"../data/interim/tokenized/offensive_tokens.txt\",\"w\", encoding = \"utf-8\") as f:\n",
    "        for i in offensive_tokens:\n",
    "            string = str(i)\n",
    "            f.write(string[1:-1]+\"\\n\")\n",
    "\n",
    "    with open(\"../data/interim/tokenized/sentiment_tokens.txt\",\"w\", encoding = \"utf-8\") as f:\n",
    "        for i in sentiment_tokens:\n",
    "            string = str(i)\n",
    "            f.write(string[1:-1]+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens and their counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tr ' ' '\\n' <../data/interim/tokenized/offensive_tokens.txt | sort | uniq -c | sort -r > ../data/interim/uniq/offensive_uniq.txt\n",
    "tr \" \" \"\\n\" <../data/interim/tokenized/sentiment_tokens.txt | sort | uniq -c | sort -r > ../data/interim/uniq/sentiment_uniq.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ten most frequent tokens in \"offensive_training\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head ../data/interim/uniq/offensive_uniq.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ten most frequent tokens in \"sentiment_training\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head ../data/interim/uniq/sentiment_uniq.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniq txt files loaded in as dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offensive uniq dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_uniq = pd.read_csv(\"../data/interim/uniq/offensive_uniq.txt\", sep=\"\\n\", names = [\"count\",\"token\"])\n",
    "off_uniq = off_uniq[\"count\"].str.split(expand=True) #split the values to get count and tokens in different columns\n",
    "off_uniq[0] = off_uniq[0].astype(int) #typecast elements in column 0 to integers\n",
    "off_uniq.replace(\"^\\'|[',]{1,2}$\", \"\", regex=True, inplace=True)  #Regex to remove the '', in the tokens they're present\n",
    "\n",
    "off_uniq\n",
    "\n",
    "\n",
    "#\"^\\d{1,5}\" for start 1-5 ints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Uniq Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_uniq = pd.read_csv(\"../data/interim/uniq/sentiment_uniq.txt\", sep=\"\\n\", names = [\"count\",\"token\"])\n",
    "sent_uniq = sent_uniq[\"count\"].str.split(expand=True) #split the values to get count and tokens in different columns\n",
    "sent_uniq[0] = sent_uniq[0].astype(int) #typecast elements in column 0 to integers\n",
    "sent_uniq.replace(\"^\\'|[',]{1,2}$\", \"\", regex=True, inplace=True)  # \"^\\d{1,5}\" for start 1-5 ints\n",
    "\n",
    "sent_uniq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type/token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Types == Amount of different Tokens in dataset\n",
    "off_types = len(off_uniq[1])\n",
    "sent_types = len(sent_uniq[1])\n",
    "print(\"Offensive Types: {}\\nSentiment types: {}\\n\".format(off_types,sent_types))\n",
    "\n",
    "#Tokens == Amount of all \"Words\" in dataset\n",
    "off_token_amount = off_uniq[0].sum()\n",
    "sent_token_amount = sent_uniq[0].sum()\n",
    "print(\"Offensive tokens, amount: {}\\nSentiment tokens, amount: {}\\n\".format(off_token_amount, sent_token_amount))\n",
    "\n",
    "#Type/token ratio (=ttratio)\n",
    "off_ttratio = off_types/off_token_amount\n",
    "sent_ttratio = sent_types/sent_token_amount\n",
    "print(\"Offensive type/token ratio: {:.4f}\\nSentiment type/token ratio: {:.4f}\".format(off_ttratio, sent_ttratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens that only occur 1, 2 or 3 times\n",
    "<ul>\n",
    "    <li>Things like Hashtags and emojis are prevalent, but they, more importantly, contain most of the types/vocabulary</li>\n",
    "    <li>Tokens that occur only once make up 58% of the types in both datasets!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Offensive types w. freq. 1 divided by total types: {:.2f}\".format(len(off_uniq.loc[off_uniq[0]==1])/off_types*100))\n",
    "print(\"Sentiment types w. freq. 1 divided by total types: {:.2f}\".format(len(sent_uniq.loc[sent_uniq[0]==1])/sent_types*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amount of types showing up x times in the offensive dataset (e.g 14000 tokens only showing up once, and so on)\n",
    "#500 Most common tokens skipped, to make plot visible\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "sns.countplot(x=0, data=off_uniq[500:]) #Sns counts the type frequency of each word, and plots it\n",
    "sns.set_style(\"darkgrid\")\n",
    "ax.tick_params('x',rotation=45, labelsize = 10) #xlabels are rotated 45 degrees and made bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noticable difference in the two datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Big difference in size, sentiment dataset over twice the amount of tokens (=library twice the size)</li>\n",
    "    <li>otherwise quite similar, in both sets the percentage of the vocabulary made up of tokens w. frq. 1 is 58%</li>\n",
    "    <ul><li>Both datasets also seem to follow Zipf's law (see below graphs)</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Statistics Consistent with Zipf's law?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log-log plot of the rank of token frequency against against the frequency in the offensive dataset\n",
    "off_uniq[\"log_frq\"] = np.log(off_uniq[0])\n",
    "off_uniq[\"log_rank\"] = np.log(off_uniq[0].rank(ascending=False))\n",
    "sns.relplot(x=\"log_rank\",y=\"log_frq\", data=off_uniq, color=\"red\", edgecolor=(0.2,0,0,0.01)).set(title=\n",
    "                \"log-log plot of frequency against rank of frequency in Offensive dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log-log plot of the rank of token frequency against against the frequency in the sentiment dataset\n",
    "sent_uniq[\"log_frq\"] = np.log(sent_uniq[0])\n",
    "sent_uniq[\"log_rank\"] = np.log(sent_uniq[0].rank(ascending=False))\n",
    "sns.relplot(x=\"log_rank\",y=\"log_frq\", data=sent_uniq, color=\"r\", edgecolor=(0.2,0,0,0.01)).set(title=\n",
    "                \"log-log plot of frequency against rank of frequency in Sentiment dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>As seen in the above plots, both datasets seem consistent with Zipf's law</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Manual Annotation & Inter-user Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating 100 random tweets for manual annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42) #Seeded for consistency\n",
    "random_tweets = random.sample(list(enumerate(sentiment_raw)),100)\n",
    "rtweet_index = [i[0] for i in random_tweets]\n",
    "\n",
    "# File-generation is commented out, as the randomness is seeded, thus Making the same \"Random\" file every time\n",
    "# with open(\"../data/interim/random_tweets.txt\",\"w\", encoding=\"utf-8\") as f:\n",
    "#     [f.write(str(i[1])+\"\\n\") for i in random_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_label = pd.read_csv('../data/raw/sentiment/train_labels.txt',header=None)\n",
    "sent_raw = pd.read_csv(\"../data/raw/sentiment/train_text.txt\",header=None, sep=\"\\n\",quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(sent_label.iloc[rtweet_index])\n",
    "# display(sent_raw.iloc[rtweet_index])\n",
    "len(sentiment_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating 100 random ints from the interval [0-2], for later testing\n",
    "\n",
    "test_labels = random.choices([0,1,2], k=100)\n",
    "with open(\"../data/interim/manual_annotation/random_test.txt\",\"w\", encoding=\"utf-8\") as f:\n",
    "    [f.write(str(i)+\"\\n\") for i in test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the manually annotated labels into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_label_list = os.listdir(\"../data/interim/manual_annotation\")\n",
    "\n",
    "df_list = []\n",
    "for i in enumerate(man_label_list):\n",
    "    temp_df = pd.read_csv(\"../data/interim/manual_annotation/\"+i[1], header=None, dtype=int)\n",
    "    df_list.append(temp_df)\n",
    "    \n",
    "all_labels = pd.concat(df_list,axis=1)\n",
    "#all_labels\n",
    "display(all_labels.eq(all_labels.iloc[:,0], axis=0).all(1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_label = all_labels.eq(all_labels.iloc[:,0], axis=0).all(1)\n",
    "print(np.sum(same_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textbf{Calculating}\\\\  A_e$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
