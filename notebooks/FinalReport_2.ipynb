{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Sentiment and offensive language analysis - Group 10 - Project 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this report we are aiming to detect offensiveness and sentimentality of different tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import difflib\n",
    "from nltk import agreement\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random #only used for generating 100 random tweets for manual labelling\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score, classification_report, roc_auc_score, roc_curve, recall_score, precision_score, f1_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All functions for the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the tokenaziation punctuations, emojois, pointless strings and characters are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(tweets):\n",
    "    \"\"\"\n",
    "    Function that takes a list of strings and returns the tokenized version of each string\n",
    "    \"\"\"\n",
    "    #counter = 0\n",
    "    #token_pat = re.compile(r'[\\w@’#]+')\n",
    "    token_pat = re.compile(r'\\w+')\n",
    "    skippable_pat = re.compile(r'[\\s\\d]+|@user')\n",
    "\n",
    "    non_white_space = re.compile(r'[^@’#\\w\\s]') #Finds characters that are not white_space nor word characters (nor @’#)\n",
    "    #print(\"these are the tweets\")\n",
    "    #print(tweets)\n",
    "    \n",
    "    # Initialise lists\n",
    "    tokens = []\n",
    "    unmatchable = []\n",
    "\n",
    "# Compile patterns for speedup\n",
    "    token_pat = re.compile(r'\\w+')\n",
    "\n",
    "    tokenlist = []\n",
    "    for i in tweets:\n",
    "        #counter = counter + 1\n",
    "        #print(counter)\n",
    "        #tokens = []\n",
    "        #unmatchable = []\n",
    "        line = i.lower()\n",
    "        #print(\"this is i: \",i)\n",
    "        \n",
    "        while line:\n",
    "            #print(\"this is the line\")\n",
    "            #print(line)\n",
    "            skippable_match = re.search(skippable_pat, line)\n",
    "            if skippable_match and skippable_match.start() == 0:\n",
    "                line = line[skippable_match.end():]\n",
    "            else:\n",
    "                token_match = re.search(token_pat, line)\n",
    "                #print(\"tokens_match\")\n",
    "                #print(token_match)\n",
    "                #print(token_match.start())\n",
    "                if token_match and token_match.start() == 0:\n",
    "                    #print(\"\\nAPPEND IS RUNNING\\n\")\n",
    "                    #print(line[:token_match.end()])\n",
    "                    tokens.append(line[:token_match.end()])\n",
    "                    line = line[token_match.end():]\n",
    "                else:\n",
    "                    unmatchable_end = len(line)\n",
    "                    if skippable_match:\n",
    "                        unmatchable_end = skippable_match.start()\n",
    "                    if token_match:\n",
    "                        unmatchable_end = min(unmatchable_end, token_match.start())\n",
    "                    unmatchable.append(line[:unmatchable_end])\n",
    "                    line = line[unmatchable_end:]\n",
    "        tokenlist.append(tokens)\n",
    "        tokens = []\n",
    "    return(tokenlist)\n",
    "\n",
    "\n",
    "def compare_tokenizers(bool):\n",
    "    if bool==True:\n",
    "        tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "        j = 0\n",
    "        for i in training_data: \n",
    "            temp = i\n",
    "            diff = difflib.context_diff(tknzr.tokenize(i),token_tweets[j])\n",
    "            #print(\"\".join(diff), end = \"\")\n",
    "            print(i,\"tknzr:\",tknzr.tokenize(i),\"\\ntokenlist:\",token_tweets[j],\"\\n\")\n",
    "            j+=1\n",
    "\n",
    "            \n",
    "def import_(classification_task, file_name):\n",
    "    with open(\"../data/raw/\"+classification_task+\"/\"+file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        temp = [int(line.strip(\"\\n\")) for line in f]\n",
    "    return(temp)\n",
    "\n",
    "\n",
    "def import_and_tokenize(classification_task, file_name):\n",
    "    with open(\"../data/raw/\"+classification_task+\"/\"+file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        temp = [line for line in f]\n",
    "    return(tokenizer(temp))\n",
    "\n",
    "\n",
    "def report_clf_stats(predicted, test, classification_task):\n",
    "    name_dict = {\"offensive\": [\"Not offensive\",\"Offensive\"], \"sentiment\": [\"Negative\", \"Neutral\", \"Positive\"]}\n",
    "    print(metrics.accuracy_score(test, predicted))\n",
    "    print(metrics.classification_report(predicted, test, target_names=name_dict[classification_task]),\"\\n\")\n",
    "    print(metrics.confusion_matrix(test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "### The Offensive Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../data/raw/offensive/train_text.txt\", 'r', encoding = \"utf-8\")\n",
    "inputlist = [line for line in f]\n",
    "f.close()\n",
    "\n",
    "training_data, validation_data = inputlist[:len(inputlist)//2], inputlist[len(inputlist)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_tweets = tokenizer(training_data)\n",
    "print(token_tweets)\n",
    "#print(token_tweets[1])\n",
    "#[print(*i) for i in token_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Comparing our own tokenizer with TweetTokenizer from nltk library\n",
    "<b>Set below value 'see_output' = True for comparison <i>(It'll run for a while)</i></b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing our own tokenizer with TweetTokenizer from nltk library\n",
    "# Set below value 'see_output' = True for comparison\n",
    "see_output = False\n",
    "compare_tokenizers(see_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus size of Offensive and sentiment training sets respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wc ../data/raw/offensive/train_text.txt\n",
    "wc ../data/raw/sentiment/train_text.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Offensive:</b> 11916 lines/tweets, 262370 words <br>\n",
    "<b>Sentiment:</b> 45615 lines/tweets, 877516 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running tokenizer function on offensive and sentiment training data to get token count right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/raw/offensive/train_text.txt\", \"r\",  encoding = \"utf-8\",) as f:\n",
    "    offensive_raw = [line for line in f]\n",
    "\n",
    "with open(\"../data/raw/sentiment/train_text.txt\", \"r\",  encoding = \"utf-8\",) as f:\n",
    "    sentiment_raw = [line for line in f]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Below cell line takes some time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offensive_tokens = tokenizer(offensive_raw)\n",
    "sentiment_tokens = tokenizer(sentiment_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The top 10 most frequent words of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://stackoverflow.com/questions/45019607/count-occurrence-of-a-list-in-a-list-of-lists\n",
    "off_uniq = pd.Series(offensive_tokens).explode().value_counts()\n",
    "sent_uniq = pd.Series(sentiment_tokens).explode().value_counts()\n",
    "\n",
    "print(\"Offensive dataset, top 10 tokens:\",\"\\n\",off_uniq[:10],\"\\n\")\n",
    "print(\"Sentiment dataset, top 10 tokens:\",\"\\n\",sent_uniq[:10])\n",
    "\n",
    "#Turning above pd.series into dataframes, for ease of use later\n",
    "#Transformation found at:https://stackoverflow.com/questions/40224319/pandas-series-to-dataframe-using-series-indexes-as-columns\n",
    "off_uniq = off_uniq.to_frame().reset_index()\n",
    "sent_uniq = sent_uniq.to_frame().reset_index()\n",
    "\n",
    "#Renaming columns in dataframes\n",
    "off_uniq.columns = [\"token\",\"count\"]\n",
    "sent_uniq.columns = [\"token\",\"count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type/token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Types == Amount of different Tokens in dataset\n",
    "off_types = len(off_uniq[\"token\"])\n",
    "sent_types = len(sent_uniq[\"token\"])\n",
    "print(\"Offensive Types: {}\\nSentiment types: {}\\n\".format(off_types,sent_types))\n",
    "\n",
    "#Tokens == Amount of all \"Words\" in dataset\n",
    "off_token_amount = off_uniq[\"count\"].sum()\n",
    "sent_token_amount = sent_uniq[\"count\"].sum()\n",
    "print(\"Offensive tokens, amount: {}\\nSentiment tokens, amount: {}\\n\".format(off_token_amount, sent_token_amount))\n",
    "\n",
    "#Type/token ratio (=ttratio)\n",
    "off_ttratio = off_types/off_token_amount\n",
    "sent_ttratio = sent_types/sent_token_amount\n",
    "print(\"Offensive type/token ratio: {:.4f}\\nSentiment type/token ratio: {:.4f}\".format(off_ttratio, sent_ttratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types that only occur 1, 2 or 3 times\n",
    "<ul>\n",
    "    <li>Things like Hashtags and misspelled nouns are prevalent, but they, more importantly, contain most of the Types in the vocabulary</li>\n",
    "    <li>Tokens that occur only once make up ~ 50% of the types in both datasets!</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Offensive types w. freq 1, 2, or 3 divided by total types: {:.2f}%\".format(\n",
    "    len(off_uniq.loc[(off_uniq[\"count\"]==1) | (off_uniq[\"count\"]==2) | (off_uniq[\"count\"]==3)])/off_types*100))\n",
    "print(\"Sentiment types w. freq 1, 2, or 3 divided by total types: {:.2f}%\".format(\n",
    "    len(sent_uniq.loc[(sent_uniq[\"count\"]==1) | (sent_uniq[\"count\"]==2) | (sent_uniq[\"count\"]==3)])/sent_types*100))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Offensive types w. freq. just 1 divided by total types: {:.2f}%\".format(len(off_uniq.loc[off_uniq[\"count\"]==1])/off_types*100))\n",
    "print(\"Sentiment types w. freq. just 1 divided by total types: {:.2f}%\".format(len(sent_uniq.loc[sent_uniq[\"count\"]==1])/sent_types*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amount of types showing up x times in the offensive dataset (e.g 14000 tokens only showing up once, and so on)<br>\n",
    "The 500 types with the highest count are skipped, to make x-axis on the plot visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of types showing up x times in the offensive dataset (e.g 14000 tokens only showing up once, and so on)\n",
    "# Skipping top 500 types, for visibility in plot (They're not impactful on the plot otherwise, the most frequent of \n",
    "#     these 500 entries is 13)\n",
    "#Plotting visual and double y-axes found at https://stackoverflow.com/questions/33179122/seaborn-countplot-with-frequencies\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "sns.countplot(x=\"count\", data=off_uniq[500:]) #Sns counts the type frequency of each word, and plots it\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.title(\"Frequency of tokens showing up x times in the Offensive dataset\")\n",
    "plt.xlabel(\"token occuring x time(s)\")\n",
    "plt.ylabel(\"sum of types occurring x time(s)\")\n",
    "ax.tick_params('x',rotation=45, labelsize = 10) #xlabels are rotated 45 degrees and made bigger\n",
    "\n",
    "# Twin axes, creating and visualising\n",
    "ax2 = ax.twinx()\n",
    "ax2.set_ylabel(\"Frequency (percent)\")\n",
    "\n",
    "# Moving the ticks and labels of y-axes to opposite sides for more visually pleasing plot\n",
    "ax2.yaxis.tick_left()\n",
    "ax.yaxis.tick_right()\n",
    "ax.yaxis.set_label_position('right')\n",
    "ax2.yaxis.set_label_position('left')\n",
    "\n",
    "# Setting appropriate limits for the y-axes, removing duplicate grid\n",
    "ax.set_ylim(0,len(off_uniq))\n",
    "ax2.set_ylim(0,100)\n",
    "ax2.grid(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noticable difference in the two datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Big difference in size, sentiment dataset over twice the amount of tokens (=library of sentiment twice the size of library of offensive language)</li>\n",
    "    <li>otherwise quite similar, in both sets the percentage of the vocabulary made up of tokens w. frq. 1 is ~ 50%</li>\n",
    "    <ul><li>Both datasets also seem to follow Zipf's law (see below graphs)</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Statistics Consistent with Zipf's law?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-log plot of the rank of token frequency against against the frequency in the offensive dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_uniq[\"log_frq\"] = np.log(off_uniq[\"count\"])\n",
    "off_uniq[\"log_rank\"] = np.log(off_uniq[\"count\"].rank(ascending=False))\n",
    "sns.relplot(x=\"log_rank\",y=\"log_frq\", data=off_uniq, color=\"red\", edgecolor=(0.2,0,0,0.01)).set(title=\n",
    "                \"log-log plot of frequency against rank of frequency in Offensive dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_uniq[\"log_frq\"] = np.log(sent_uniq[\"count\"])\n",
    "sent_uniq[\"log_rank\"] = np.log(sent_uniq[\"count\"].rank(ascending=False))\n",
    "sns.relplot(x=\"log_rank\",y=\"log_frq\", data=sent_uniq, color=\"r\", edgecolor=(0.2,0,0,0.01)).set(title=\n",
    "                \"log-log plot of frequency against rank of frequency in Sentiment dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>As seen in the above plots, both datasets seem consistent with Zipf's law</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Manual Annotation & Inter-user Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating 100 random tweets for manual annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42) #Seeded for consistency\n",
    "random_tweets = random.sample(list(enumerate(sentiment_raw)),100)\n",
    "rtweet_index = [i[0] for i in random_tweets]\n",
    "\n",
    "# #File-generation is commented out, as the randomness is seeded, thus Making the same \"Random\" file every time\n",
    "# with open(\"../data/interim/random_tweets.txt\",\"w\", encoding=\"utf-8\") as f:\n",
    "#     [f.write(str(i[1])+\"\\n\") for i in random_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_label = pd.read_csv('../data/raw/sentiment/train_labels.txt',header=None)\n",
    "sent_raw = pd.read_csv(\"../data/raw/sentiment/train_text.txt\",header=None, sep=\"\\n\",quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the manually annotated labels into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_labels = pd.read_csv(\"../data/interim/manual_annotation/all_combined.csv\", delimiter=\",\") #All manual labels\n",
    "man_labels = man_labels.iloc[:,:-1] #Not using the _A0_value column from the file\n",
    "display(man_labels) #The manually annotated labels, put into a dataframe\n",
    "\n",
    "same_label = man_labels.eq(man_labels.iloc[:,0], axis=0).all(1) #Finding where all annotators agree on a label\n",
    "print(\"# of equal labels:\",np.sum(same_label))\n",
    "obs_agreement = np.sum(same_label)/len(man_labels.iloc[:,0])\n",
    "print(\"observed agreement:\",obs_agreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Chance-corrected agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting manual label answers to calculate Scott's pi, Fleiss' kappa with nltk.agreement\n",
    "tweets_len = len(man_labels.iloc[:,0])\n",
    "formatted_answers = [] #Formatting of only the manually annotated data\n",
    "for column in range(len(man_labels.columns)):\n",
    "    for tweet_num in range(tweets_len):\n",
    "        formatted_answers.append([column+1,tweet_num,man_labels.iloc[tweet_num,column]])\n",
    "\n",
    "#adding the \"True\" labels to all_formatted:\n",
    "all_formatted = formatted_answers.copy() #Formatting of BOTH the manually annotated data AND the \"True\" Annotation of the data\n",
    "true_label_list = list(sent_label.iloc[rtweet_index][0])\n",
    "for i in range(tweets_len):\n",
    "    all_formatted.append([len(man_labels.columns)+1,i,true_label_list[i]])\n",
    "    \n",
    "print(\"lenght of formatted_answers:\",len(formatted_answers))\n",
    "print(\"length of all_formatted:\",len(all_formatted)) #should be 100 characters longer than the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chance-corrected for just the manual labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inter-annotator agreement values are almost 0.4, which can be decided as fair or moderate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note that in the nltk.agreement documentation: https://www.nltk.org/_modules/nltk/metrics/agreement.html\n",
    "the returned value is the chance-corrected agreement, not just A_e.\n",
    "\"\"\"\n",
    "ratingtask = agreement.AnnotationTask(data=formatted_answers)\n",
    "print(\"Scott's pi: {:.4f}\\nCohen's kappa: {:.4f}\\nFleiss' kappa: {:.4f}\".format(ratingtask.pi(),ratingtask.kappa(),ratingtask.multi_kappa()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chance-corrected for both the manual labels AND the \"true\" labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_label_rating = agreement.AnnotationTask(data=all_formatted)\n",
    "print(\"Scott's pi: {:.4f}\\nCohen's kappa: {:.4f}\\nFleiss' kappa: {:.4f}\".format(all_label_rating.pi(),all_label_rating.kappa(),all_label_rating.multi_kappa()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the tweets with agreeing/disagreeing manual labels for later discussion, saved to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_label[same_label==False]\n",
    "manual_tweets = sent_raw.loc[rtweet_index]\n",
    "\n",
    "#tweets labels disagree on\n",
    "annotation_disagree = manual_tweets.iloc[np.where(same_label==False)]\n",
    "display(annotation_disagree[:10])#Showing the 10 first tweets with disagreeing manual annotation\n",
    "\n",
    "#tweets labels agree on\n",
    "annotation_agree = manual_tweets.iloc[np.where(same_label==True)]\n",
    "\n",
    "# File creation commented out\n",
    "# annotation_disagree.to_csv(\"../data/interim/man_anno_disagree.txt\", header=None, index=False)\n",
    "# annotation_agree.to_csv(\"../data/interim/man_anno_agree.txt\", header=None, index=False)\n",
    "\n",
    "#man_labels[same_label==True][\"anno_1\"][:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the Observed Agreement result of the manual annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the manual annotation we found in case:\n",
    "- Only 34% of the cases we totally agreed 34% \n",
    "- 43% of the cases one of us had different opinion\n",
    "- 23% of the cases (this means 0.5 Observed Agreement result) we couldn't decide obviously the sentiment of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hide errors\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "man_labels.reset_index()\n",
    "man_labels[\"AO\"] = 1.1\n",
    "for i in range(len(man_labels)):\n",
    "    l = []\n",
    "    rowData = man_labels.loc[ i , : ]\n",
    "    l.append(rowData[0])\n",
    "    l.append(rowData[1])\n",
    "    l.append(rowData[2])\n",
    "    l.append(rowData[3])\n",
    "    occurence_count = Counter(l)\n",
    "    same_counter = occurence_count.most_common(1)[0][1]\n",
    "    res=occurence_count.most_common(1)[0][0]\n",
    "    ao = same_counter/4\n",
    "    #print(ao)\n",
    "    man_labels[\"AO\"][i] = ao\n",
    "    #print(\"Variance of sample set is % s\"  %(statistics.variance(l)),\"AO number is : \", ao, \"winner is :\", res)\n",
    "    None\n",
    "\n",
    "plt.hist(man_labels[\"AO\"], label='linear', bins=3)\n",
    "plt.title('Observed Agreement results of manual annotation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohen's kappa score heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_and_true = man_labels.copy()\n",
    "man_and_true = man_and_true.iloc[:,:4]\n",
    "man_and_true[\"true\"] = true_label_list\n",
    "\n",
    "annotator_np = np.empty((5,5))\n",
    "\n",
    "for i in range(len(man_and_true.columns)):\n",
    "    for j in range(i, len(man_and_true.columns)):\n",
    "        l1 = list(man_and_true.iloc[:,i])\n",
    "        l2 = list(man_and_true.iloc[:,j])\n",
    "        score = cohen_kappa_score(l1,l2)\n",
    "        annotator_np[i,j] = score\n",
    "        annotator_np[j,i] = score\n",
    "#         if j==4:\n",
    "#             print(\"Anno_{}, True:\\n\".format(i+1),score,\"\\n\")\n",
    "#         else:\n",
    "#             print(\"Anno_{}, Anno_{}:\\n\".format(i+1,j+1),score,\"\\n\")\n",
    "\n",
    "#print(annotator_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.heatmap(data=annotator_np, annot=True,\n",
    "            xticklabels= [\"Annotator 1\",\"Annotator 2\",'Annotator 3',\"Annotator 4\",\"'True' labels\"],\n",
    "           yticklabels = [\"Annotator 1\",\"Annotator 2\",'Annotator 3',\"Annotator 4\",\"'True' labels\"])\n",
    "ax.tick_params('x',rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running inter-annotator agreement without Annotator 3:\n",
    "just_three_annos = list(filter(lambda x: x[0] != 3, all_formatted))\n",
    "just_three_annos\n",
    "three_anno_rating = agreement.AnnotationTask(data=just_three_annos)\n",
    "print(\"Scott's pi: {:.4f}\\nCohen's kappa: {:.4f}\\nFleiss' kappa: {:.4f}\".format(three_anno_rating.pi(),three_anno_rating.kappa(),three_anno_rating.multi_kappa()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model on Offensive Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(tokenizer = lambda x: x, lowercase = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in offensive x-train, x-test, y-train, y-test\n",
    "\n",
    "# x-train\n",
    "ox_train = import_and_tokenize(\"offensive\", \"train_text.txt\")\n",
    "\n",
    "# x-test\n",
    "ox_test = import_(\"offensive\", \"train_labels.txt\")\n",
    "\n",
    "# y-train\n",
    "oy_train = import_and_tokenize(\"offensive\", \"test_text.txt\")\n",
    "\n",
    "# y-test\n",
    "oy_test = import_(\"offensive\", \"test_labels.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running Count_vectorizor (Pipeline for the coming commands)\n",
    "count_ox_train = count_vec.fit_transform(ox_train)\n",
    "#count_ox_train\n",
    "#Running tf_idf on off_train to \"balance\" tweets\n",
    "tf_idf_transformer = TfidfTransformer(use_idf = False)\n",
    "tf_off_train = tf_idf_transformer.fit_transform(count_ox_train)\n",
    "#tf_off_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SGDClassifier(loss=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training model\n",
    "off_clf = classifier.fit(tf_off_train, ox_test)\n",
    "\n",
    "#Preparing validation data\n",
    "off_pred = count_vec.transform(oy_train)\n",
    "tf_off_pred = tf_idf_transformer.transform(off_pred)\n",
    "\n",
    "#Fitting validation data over model\n",
    "off_predicted = off_clf.predict(tf_off_pred)\n",
    "\n",
    "# % of answers gotten right\n",
    "sum((off_predicted == oy_test)) / len(oy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(off_predicted, oy_test, target_names=[\"Not offensive\",\"Offensive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(oy_test, off_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, The recall is very close to 1 for non-offensive tweets and very close to 0 for offensive tweets.<br>\n",
    "This means that the model predicts that most of the tweets are not offensive, and the only reason for our relatively high accuracy is that the training data is unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking offensive validation data compared to offensive predicted data\n",
    "np_list_off = np.array(off_predicted)\n",
    "np_off_validation = np.array(oy_test)\n",
    "np_off_train = np.array(ox_test)\n",
    "\n",
    "print(\"Number of predicted non-offensive tweets: {}\\nNumber of predicted offensive tweets: {}\\n\".format(\n",
    "len(np_list_off[np.where(np_list_off == 0)]),\n",
    "len(np_list_off[np.where(np_list_off == 1)])))\n",
    "\n",
    "print(\"Number of actual non-offensive tweets: {}\\nNumber of actual offensive tweets: {}\\n\".format(\n",
    "len(np_off_validation[np.where(np_off_validation == 0)]),\n",
    "len(np_off_validation[np.where(np_off_validation == 1)])))\n",
    "\n",
    "print(\"Number of training non-offensive tweets: {}\\nNumber of training offensive tweets: {}\".format(\n",
    "len(np_off_train[np.where(np_off_train == 0)]),\n",
    "len(np_off_train[np.where(np_off_train == 1)])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models for the offensive dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline for sgdclassifier\n",
    "sgd_clf = Pipeline([\n",
    "     ('vec', CountVectorizer(tokenizer = lambda x: x, lowercase = False, ngram_range=(1,3),\n",
    "                            max_df = 0.7, min_df = 4, max_features = 1000)),\n",
    "     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "     ('clf', SGDClassifier(loss=\"log\")),\n",
    "])\n",
    "\n",
    "sgd_clf.fit(ox_train, ox_test)\n",
    "sgd_predicted2 = sgd_clf.predict(oy_train)\n",
    "#sgd_predicted2\n",
    "\n",
    "report_clf_stats(sgd_predicted2, oy_test, \"offensive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Highest Achieved accuracy score for SGDClassifier: 78.6% </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB\n",
    "multinb_clf = Pipeline([\n",
    "     ('vec', CountVectorizer(tokenizer = lambda x: x, lowercase = False)),\n",
    "     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "     ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "multinb_clf.fit(ox_train, ox_test)\n",
    "multinb_predict = multinb_clf.predict(oy_train)\n",
    "\n",
    "report_clf_stats(multinb_predict, oy_test, \"offensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ComplementNB\n",
    "complement_clf = Pipeline([\n",
    "     ('vec', CountVectorizer(tokenizer = lambda x: x, lowercase = False)),\n",
    "     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "     ('clf', ComplementNB()),\n",
    "])\n",
    "\n",
    "complement_clf.fit(ox_train, ox_test)\n",
    "complement_predict = complement_clf.predict(oy_train)\n",
    "\n",
    "report_clf_stats(complement_predict, oy_test, \"offensive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes time to run SVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "SVC_clf = Pipeline([\n",
    "     ('vec', CountVectorizer(tokenizer = lambda x: x, lowercase = False)),\n",
    "     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "     ('clf', SVC(kernel='poly', degree = 3)),\n",
    "])\n",
    "\n",
    "SVC_clf.fit(ox_train, ox_test)\n",
    "SVC_predict = SVC_clf.predict(oy_train)\n",
    "\n",
    "report_clf_stats(SVC_predict, oy_test, \"offensive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers for the sentiment (multiclass) task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in Sentiment x-train, x-test, y-train, y-test\n",
    "\n",
    "# x-train\n",
    "sx_train = import_and_tokenize(\"sentiment\", \"train_text.txt\")\n",
    "\n",
    "# x-test\n",
    "sx_test = import_(\"sentiment\", \"train_labels.txt\")\n",
    "\n",
    "# y-train\n",
    "sy_train = import_and_tokenize(\"sentiment\", \"test_text.txt\")\n",
    "\n",
    "# y-test\n",
    "sy_test = import_(\"sentiment\", \"test_labels.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes on Sentiment analysis\n",
    "\n",
    "multinb_clf.fit(sx_train, sx_test)\n",
    "sent_multinb = multinb_clf.predict(sy_train)\n",
    "\n",
    "report_clf_stats(sent_multinb, sy_test, \"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complement on Sentiment\n",
    "\n",
    "complement_clf.fit(sx_train, sx_test)\n",
    "sent_complement = complement_clf.predict(sy_train)\n",
    "\n",
    "report_clf_stats(sent_complement, sy_test, \"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD on Sentiment\n",
    "\n",
    "sgd_clf.fit(sx_train, sx_test)\n",
    "sent_sgd = sgd_clf.predict(sy_train)\n",
    "\n",
    "report_clf_stats(sent_sgd, sy_test, \"sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing validation x- and y- from both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offensive validation:\n",
    "\n",
    "# y-train\n",
    "oval_train = import_and_tokenize(\"offensive\",\"val_text.txt\")\n",
    "\n",
    "# y-test\n",
    "oval_true = import_(\"offensive\", \"val_labels.txt\")\n",
    "\n",
    "# Sentiment validation\n",
    "\n",
    "# y-train\n",
    "sval_train = import_and_tokenize(\"sentiment\",\"val_text.txt\")\n",
    "\n",
    "# y-test\n",
    "sval_true = import_(\"sentiment\",\"val_labels.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offensive validation prediction\n",
    "<i>Using SGDClassifier, as that gave the highest accuracy and F1 score </i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oval_pred = count_vec.transform(oval_train)\n",
    "tf_oval_pred = tf_idf_transformer.transform(oval_pred) #transforming data, as we didn't have a pipeline for this exact model\n",
    "\n",
    "oval_sgd = off_clf.predict(tf_oval_pred) # model to use\n",
    "\n",
    "report_clf_stats(oval_sgd, oval_true,\"offensive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment validation prediction\n",
    "<i>Using Complement, as that gave the highest accuracy and F1 score </i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sval_complement = complement_clf.predict(sval_train)\n",
    "\n",
    "report_clf_stats(sval_complement, sval_true, \"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
